{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  #import numpy package as np\n",
    "import matplotlib.pyplot as plt  #import pyplot from matplotlib as plt\n",
    "from scipy.optimize import curve_fit  #import the curve_fit function from scipy.optimize\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import scipy.stats\n",
    "import math\n",
    "sns.set(style='darkgrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizing function fits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Residual\n",
    "Residual = 0: Model is perfect fit to observation\n",
    "\n",
    "Residual >0: Model underestimates \n",
    "\n",
    "Residual <0: Model overestimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def residual_err(y_obv, y_fit, x_obv):\n",
    "    residual_err = y_obv - y_fit\n",
    "    sns.scatterplot(x=x_obv, y=residual_err, alpha=0.5)\n",
    "    sns.lineplot(x=x_obv, y=0, color='k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Absolute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def absolute_err(y_obv, y_fit):\n",
    "    residual_err = y_obv - y_fit\n",
    "    absolute_err = abs(residual_err)\n",
    "    return (absolute_err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relative_err(y_obv, y_fit):\n",
    "    residual_err = y_obv - y_fit\n",
    "    absolute_err = abs(residual_err)\n",
    "    relative_err = absolute_err / abs(y_obv)\n",
    "    return (relative_err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Squared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squared_err(y_obv, y_fit):\n",
    "    residual_err = y_obv - y_fit\n",
    "    squared_err = residual_err**2\n",
    "    return (squared_err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sum of Squared\n",
    "\n",
    "\n",
    "This quantity is very important in curve fitting as often it is used by fitting algorithms to select the parameters that \"best\" fit the data. For example, the curve_fit function used in the example above chooses the values of 𝑚 and 𝑐 that minimize the sum of the squared errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sumOfSquared(y_obv, y_fit):\n",
    "    residual_err = y_obv - y_fit\n",
    "    squared_err = residual_err**2\n",
    "    sumOfSquared = sum(squared_err)\n",
    "    return (sumOfSquared)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Root-Mean-Squared-Error (RMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RMSE_fit(y_obv, y_fit):\n",
    "    residual_err = y_obv - y_fit\n",
    "    squared_err = residual_err**2\n",
    "    RMSE = (squared_err.mean())**0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coefficient of Determination\n",
    "\n",
    "The coefficient of determination is always a value between 0 and 1 and the closer it is to 1, the better the fit is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coeffOfDetermination(y_obv, y_fit):\n",
    "    \"\"\"ss_res is the MSE of your guesses using the model, ss_total is the MSE without it.\n",
    "    So their ratio is the fraction of MSE that remains if you use the model, and R2 is the fraction of MSE the model eliminates.\"\"\"\n",
    "    residual_err = y_obv - y_fit\n",
    "    squared_err = residual_err**2\n",
    "    ss_res = sum(squared_err)\n",
    "    y_mean = np.nanmean(y_obv)\n",
    "    ss_total = sum((y_fit - y_mean)**2)\n",
    "    coeffOfDetermination = 1 - (ss_res / ss_total)\n",
    "    return (coeffOfDetermination)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear\n",
    "\n",
    "\\begin{equation}y(x) = mx + c\\end{equation}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linearModel(x, m, c):\n",
    "    \"\"\"𝑦(𝑥)=𝑚𝑥+𝑐\"\"\"\n",
    "    return (m * x + c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quadratic\n",
    "\n",
    "\\begin{equation}y(x) = a_{1}x^2 + a_{2}x + a_{3}\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quadraticModel(x, a1, a2, a3):\n",
    "    \"\"\"𝑦(𝑥)=𝑎1𝑥2+𝑎2𝑥+𝑎3\"\"\"\n",
    "    return (a1 * x**2 + a2 * x + a3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exponential decay\n",
    "\\begin{equation}y(x)=a+b\\exp(-cx)\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expDecayBaseModel(x, a, b, c):\n",
    "    \"\"\"𝑦(𝑥)=𝑎+𝑏exp(−𝑐𝑥)\"\"\"\n",
    "    return (a + b * np.exp(-c * x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expDecayCeilingModel(x, a, b, c):\n",
    "    \"\"\"𝑦(𝑥)=𝑎-𝑏exp(−𝑐𝑥)\"\"\"\n",
    "    return (a - b * np.exp(-c * x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exponential growth\n",
    "$$y(x)=a\\exp(bx) + c$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expGrowthModel(x, a, b, c):\n",
    "    \"\"\"𝑦(𝑥)=𝑎exp(𝑏𝑥)+𝑐\"\"\"\n",
    "    return (a * np.exp(b * x) + c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Moving averages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Moving Average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def movingAverage (data, N):\n",
    "    \"\"\" Accepts a pandas data column\"\"\"\n",
    "    return (data.rolling(window=N).mean().iloc[N-1:].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def movingAverage2 (data, N, win_type):\n",
    "    \"\"\" Accepts a pandas data column\"\"\"\n",
    "    return (data.rolling(window=N, win_type=win_type).mean().iloc[N-1:].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weighted Moving Average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weightedMovingAverage(\n",
    "        values, weights\n",
    "):  #define a function called movingaverage that has arguments called values and weights\n",
    "    smas = np.convolve(values, weights,\n",
    "                       'valid')  #convolve the data with the weights\n",
    "    return smas  #output the filtered data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exponential Moving Average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expoMovingAverage(data,N):\n",
    "    data.ewm(span=N, adjust=False).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normal distribution\n",
    "$$f(x)=\\frac{1}{\\sqrt{2\\pi s^2}}\\exp\\left(-\\frac{(x-l)^2}{2s^2}\\right),$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats #import the scipy stats package\n",
    "def normDist(x,mean,std):\n",
    "    return(stats.norm.pdf(x,mean,std)) #calculate the y values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log distribution\n",
    "$$f(x)=\\frac{1}{x-l}\\frac{1}{\\sigma\\sqrt{2\\pi}}\\exp\\left(-\\frac{(\\log((x-l)/s))^2}{2\\sigma^2}\\right),$$\n",
    "\n",
    "This equation is equivalent to the original definition only if $l=0$ and with $s=e^{~\\mu}$. \n",
    "\n",
    "The additional parameter $l$ allows you to shift the distribution left or right which means that it will be defined over the range $(l,\\infty)$ rather than $(0,\\infty)$.\n",
    "\n",
    "distribution is only defined over the range (0,∞)(0,∞) and so is suitable for modelling variables that must be positive\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logDist(x, std, shift, mean):\n",
    "    return (stats.lognorm.pdf(x,s=std, loc= shift, scale=np.exp(mean)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Percentiles & Percentile ranks\n",
    "\n",
    "To summarize, Percentile Rank takes a value and computes its percentile rank in a set of values; Percentile takes a percentile rank and computes the corresponding value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PercentileRank(scores, your_score):\n",
    "    count = 0\n",
    "    for score in scores:\n",
    "        if score <= your_score:\n",
    "            count += 1\n",
    "            percentile_rank = 100.0 * count / len(scores)\n",
    "    return percentile_rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Percentile(scores, percentile_rank):\n",
    "    scores.sort()\n",
    "    index = percentile_rank * (len(scores)-1) // 100\n",
    "    return scores[index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CDF\n",
    "The Cumulative Distribution Function (CDF) is almost the same as PercentileRank. The only difference is that the result is 0-1 instead of 0-100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EvalCdf(sample, x):\n",
    "    count = 0.0\n",
    "    for value in sample:\n",
    "        if value <= x:\n",
    "            count += 1\n",
    "\n",
    "    prob = count / len(sample)\n",
    "    return prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log Norm Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lognorm_dist_cdf(data):\n",
    "    from scipy.stats import lognorm\n",
    "    x = np.linspace(min(data), max(data), len(data))  #define a set of x values\n",
    "    mu, std, shift = lognorm.fit(data)\n",
    "    y = lognorm.cdf(x, mu, std, shift)  #calculate the y values\n",
    "    sns.set_style('darkgrid')\n",
    "    plt.plot(x, y, '-b')  #plot normal cumulative distribution function\n",
    "    plt.xlabel('x')  #label horizontal axis\n",
    "    plt.ylabel('F(x)')\n",
    "    plt.hist(data, 40, density=True, cumulative=True, color='g')  #plot the histogram of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exponential Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exp_dist_cdf(data):\n",
    "    from scipy.stats import expon\n",
    "    x = np.linspace(min(data), max(data), len(data))  #define a set of x values\n",
    "    mu, std = expon.fit(data)\n",
    "    y = expon.cdf(x, loc=mu, scale=std)  #calculate the y values\n",
    "    sns.set_style('darkgrid')\n",
    "    plt.plot(x, y, '-b')  #plot normal cumulative distribution function\n",
    "    plt.xlabel('x')  #label horizontal axis\n",
    "    plt.ylabel('F(x)')\n",
    "    plt.hist(data, 40, density=True, cumulative=True, color='g')  #plot the histogram of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normal Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_dist_cdf(data):\n",
    "    from scipy.stats import norm\n",
    "    x = np.linspace(min(data), max(data), len(data))  #define a set of x values\n",
    "    mu, std = norm.fit(data)\n",
    "    y = norm.cdf(x, loc=mu, scale=std)  #calculate the y values\n",
    "    sns.set_style('darkgrid')\n",
    "    plt.plot(x, y, '-b')  #plot normal cumulative distribution function\n",
    "    plt.xlabel('x')  #label horizontal axis\n",
    "    plt.ylabel('F(x)')\n",
    "    plt.hist(data, 40, density=True, cumulative=True, color='g')  #plot the histogram of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chi-Squared Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chi2_dist_cdf(data):\n",
    "    from scipy.stats import chi2\n",
    "    mu, std, shift = chi2.fit(data)\n",
    "    x = np.linspace(min(data), max(data), len(data))  #define a set of x values\n",
    "    y = chi2.cdf(x, mu, std, shift)  #calculate the y values\n",
    "    sns.set_style('darkgrid')\n",
    "    plt.plot(x, y, '-b')  #plot normal cumulative distribution function\n",
    "    plt.xlabel('x')  #label horizontal axis\n",
    "    plt.ylabel('F(x)')\n",
    "    plt.hist(data, 40, density=True, cumulative=True, color='g')  #plot the histogram of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PDF Model Distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exponential Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expo_dist_model(data):\n",
    "    from scipy.stats import expon\n",
    "    mu, std = expon.fit(data)\n",
    "    # Plot the histogram.\n",
    "    plt.hist(data, bins=40, density=True, alpha=0.6, color='g')\n",
    "\n",
    "    # Plot the PDF.\n",
    "    xmin, xmax = plt.xlim()\n",
    "    x = np.linspace(xmin, xmax, 100)\n",
    "    p = expon.pdf(x, mu, std)\n",
    "    sns.set_style('darkgrid')\n",
    "    plt.figure(1)\n",
    "    plt.plot(x, p, 'k', linewidth=2)\n",
    "    title = \"Fit results: mu = %.2f,  std = %.2f\" % (mu, std)\n",
    "    plt.title(title)\n",
    "    plt.figure(2)\n",
    "    scipy.stats.probplot(data, plot=plt, dist=expon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normal Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_dist_model(data):\n",
    "    '''http://www.psychwiki.com/wiki/How_do_I_determine_whether_my_data_are_normal%3F'''\n",
    "    from scipy.stats import norm\n",
    "    mu, std = norm.fit(data)\n",
    "    # Plot the histogram.\n",
    "    plt.hist(data, bins=40, density=True, alpha=0.6, color='g')\n",
    "    # Plot the PDF.\n",
    "    xmin, xmax = plt.xlim()\n",
    "    x = np.linspace(xmin, xmax, 100)\n",
    "    p = norm.pdf(x, mu, std)\n",
    "    sns.set_style('darkgrid')\n",
    "    plt.figure(1)\n",
    "    plt.plot(x, p, 'k', linewidth=2)\n",
    "    title = \"Fit results: mu = %.2f,  std = %.2f\" % (mu, std)\n",
    "    plt.title(title)\n",
    "    plt.figure(2)\n",
    "    scipy.stats.probplot(data, plot=plt, dist=norm)\n",
    "    \n",
    "    alpha=0.05\n",
    "    k_stat, p = stats.normaltest(data)  \n",
    "    if p < alpha:  # null hypothesis: x comes from a normal distribution\n",
    "        print(\"The null hypothesis can be rejected\")\n",
    "    else:\n",
    "        print(\"The null hypothesis cannot be rejected, we learned nothing\")\n",
    "\n",
    "    print(\"The test statistic =\", k_stat, \"and the p-value =\", p)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log Norm Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lognorm_dist_model(data):\n",
    "    from scipy.stats import lognorm\n",
    "    mu, std, shift = lognorm.fit(data)\n",
    "    # Plot the histogram.\n",
    "    plt.hist(data, bins=40, density=True, alpha=0.6, color='g')\n",
    "\n",
    "    # Plot the PDF.\n",
    "    xmin, xmax = plt.xlim()\n",
    "    x = np.linspace(xmin, xmax, 100)\n",
    "    p = lognorm.pdf(x, mu, std, shift)\n",
    "    sns.set_style('darkgrid')\n",
    "    plt.figure(1)\n",
    "    plt.plot(x, p, 'k', linewidth=2)\n",
    "    title = \"Fit results: mu = %.2f,  std = %.2f\" % (mu, std)\n",
    "    plt.title(title)\n",
    "    plt.figure(2)\n",
    "    scipy.stats.probplot(data, plot=plt, dist=lognorm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chi-Squared Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import chi2\n",
    "def chi2_dist_model(data):\n",
    "    from scipy.stats import chi2\n",
    "    mu, std, shift = chi2.fit(data)\n",
    "    # Plot the histogram.\n",
    "    plt.hist(data, bins=40, density=True, alpha=0.6, color='g')\n",
    "\n",
    "    # Plot the PDF.\n",
    "    xmin, xmax = plt.xlim()\n",
    "    x = np.linspace(xmin, xmax, 100)\n",
    "    p = chi2.pdf(x, mu, std, shift)\n",
    "    sns.set_style('darkgrid')\n",
    "    plt.figure(1)\n",
    "    plt.plot(x, p, 'k', linewidth=2)\n",
    "    title = \"Fit results: mu = %.2f,  std = %.2f\" % (mu, std)\n",
    "    plt.title(title)\n",
    "    plt.figure(2)\n",
    "    scipy.stats.probplot(data, plot=plt, dist=chi2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cost functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Squared error cost function (for linear regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://miro.medium.com/max/473/1*VanG05Ab6yknqJ2bRGFzrQ.png\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squaredErrorCost(theta,x,y_obv):\n",
    "    ''' Only works for single theta (gradient)'''\n",
    "    m = len(y)\n",
    "    predictions = x.dot(theta)\n",
    "    cost = (1/(2*m)) * sumOfSquared(y, predictions)\n",
    "    return cost\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Measuring skewness\n",
    "Just check if it's positive or negative\n",
    "- positive = right\n",
    "- negative = left"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Raw Moment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RawMoment(data, k):\n",
    "    '''k = 1 --> mean'''\n",
    "    return sum(x**k for x in data) / len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Central Moment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CentralMoment(data, k):\n",
    "    '''k = 2 --> variance'''\n",
    "    mean = RawMoment(data, 1)\n",
    "    return sum((x - mean)**k for x in data) / len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardized Moment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def StandardizedMoment(data, k):\n",
    "    var = CentralMoment(data, 2)\n",
    "    std = math.sqrt(var)\n",
    "    return CentralMoment(data, k) / std**k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pearson's median skewness coefficient\n",
    "\n",
    "Measure of skewness based on the difference between the sample mean and median\n",
    "- Another way to evaluate the asymmetry of a distribution is to look at the relationship between the mean and median. Extreme values have more effect on the mean than the median, so in a distribution that skews left, the mean is less than the median. In a distribution that skews right, the mean is greater.\n",
    "\n",
    "- Pearson's median skewness is based on a computed mean and variance, so it is also susceptible to outliers, but since it does not depend on a third moment, it is somewhat more robust than sample skewness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PearsonMedianSkewness(data):\n",
    "    median = np.nanmedian(data)\n",
    "    mean = np.nanmean(data)\n",
    "    std = np.nanstd(data)\n",
    "    gp = 3 * (mean - median) / std\n",
    "    return gp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Covariance\n",
    "- NumPy and pandas also provide implementations of covariance, but both of them apply a correction for small sample sizes that we have not covered yet, and np.cov returns a covariance matrix, which is more than we need for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Cov(data1, data2):\n",
    "    data1 = np.asarray(data1)\n",
    "    data2 = np.asarray(data2)\n",
    "    mean1 = np.mean(data1)\n",
    "    mean2 = np.mean(data2)\n",
    "    cov = np.dot(data1-mean1, data2-mean2) / len(data1)\n",
    "    return cov"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pearson product-moment correlation coefficient (for linear)\n",
    "\n",
    "- Pearson's correlation is near 0, it is tempting to conclude that there is no elationship between the variables, but that conclusion is not valid. Pearson's correlation only measures linear relationships. If there's a nonlinear relationship, it understates its strength."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can modify the data such that it becomes normalized - e.g. if data follows a log-norm dist, \n",
    "# you can log data1/data2 to get a 'normal distribution' without too much outliers\n",
    "\n",
    "def pearsonCorr(data1, data2):\n",
    "    data1 = pd.Series(data1)\n",
    "    data2 = pd.Series(data2)\n",
    "    corr = data1.corr(data2, method='pearson')\n",
    "    sns.set_style('darkgrid')\n",
    "    sns.scatterplot(data1,data2)\n",
    "    title = \"Pearson correlation = %.2f\" % (corr)\n",
    "    plt.title(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spearman rank correlation coefficient\n",
    "- More robust to outliers than Pearson\n",
    "- Better for non-linear relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spearmanCorr(data1, data2):\n",
    "    data1 = pd.Series(data1)\n",
    "    data2 = pd.Series(data2)\n",
    "    return data1.corr(data2, method='spearman')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hypothesis testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "307.2px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
